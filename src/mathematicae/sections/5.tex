%------- 5.1
\subsection{Matrizes}
    %--- 5.1.1
    \subsubsection{Definição}
        Matriz é toda tabela formada por números reais, caracterizada pelo número de linhas e colunas, respectivamente, contadas de cima para baixo e da esquerda para direita:
        \[ (x, y, i, j \in \mathbb{N}) \]
        \[ M_{y \times x} = \begin{bmatrix} a_{1 \; 1} & & a_{1 \; x} \\ & \cdots & \\ a_{y \; 1} &  & a_{y \; x} \end{bmatrix} \ | \ a_{i \; j} \in \mathbb{R} \ \forall i \in \mathbb{N}^y \wedge  \forall j \in \mathbb{N}^x \]
    %--- 5.1.2
    \subsubsection{Classificações}
        \begin{description}
            \item[Matriz Nula:] é toda matriz onde todos os seus elementos são iguais a zero:
            \[ M_{y \times x} = \{a_{i \; j} = 0 \ | \ \forall i \in \mathbb{N}^y \wedge  \forall j \in \mathbb{N}^x \} \]
            \item[Matriz Linha:] é toda matriz que composta por uma única linha. \eg
            \[ M_{1 \times x} = \begin{bmatrix} a_{1 \; 1} & \cdots & a_{1 \; x} \end{bmatrix} \]
            \item[Matriz Coluna:] é toda matriz que composta por uma única coluna. \eg
            \[ M_{y \times 1} = \begin{bmatrix} a_{1 \; 1} \\ \vdots \\ a_{y \; 1} \end{bmatrix} \]
            \item[Matriz Quadrada:] é toda matriz com o mesmo número de linhas e colunas, onde este número é apontado como o seu grau. \eg
            \[ M_{n \times n} = \begin{bmatrix} a_{1 \; 1} & & a_{1 \; n} \\ & \cdots & \\ a_{n \; 1} &  & a_{n \; n} \end{bmatrix} \]
            Na matriz quadrada denota-se a importância de suas duas diagonais, a principal e secundária, respectivamente:
            \[ a_{i \; j} \in M_{n \times n} \ | \ i = j \hphantom{---} a_{i \; j} \in M_{n \times n} \ | \ i + j = n + 1 \]
            \item[Matriz Diagonal:] é toda matriz quadrada onde os elementos não pertencentes a diagonal principal são nulos. \eg
            \[ \begin{bmatrix} a_{1 \; 1} & 0 & 0 \\ 0 & a_{2 \; 2} & 0 \\ 0 & 0 & a_{3 \; 3} \end{bmatrix} \]
            \item[Matriz Identidade:] é toda matriz quadrada onde os elementos da diagonal principal são iguais a 1, enquanto todos os outros elementos são nulos. \eg
            \[ I_3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \]
            \item[Matriz Triangular:] toda matriz quadrada onde os elementos acima ou abaixo da diagonal principal são nulos:
            \[ a_{i \; j} = 0 \ \forall i < j \vee j > i \]
        \end{description}
    %--- 5.1.3
    \subsubsection{Adição}
        A soma de duas matrizes (necessariamente de mesma dimensão) é feita pela soma de seus respectivos termos:
        \[ A_{y \times x} + B_{y \times x} = C_{y \times x} \ | \ c_{i \; j} = b_{i \; j} + a_{i \; j} \]
    %--- 5.1.4
    \subsubsection{Multiplicação}
        \begin{description}
            \item[Por um Coeficiente:] a multiplicação de uma matriz por um número é feita multiplicando cada um de seus elementos pelo coeficiente: 
            \[ A_{y \times x} \cdot k = B_{y \times x} \ | \ b_{i \; j} = k \cdot a_{i \; j} \]
            \item[Por Outra Matrix:] a multiplicação de duas matrizes (necessariamente tendo o número de colunas da primeira igual ao número de linhas da segunda) resulta numa matriz contendo a somatório do produto dos elementos ordenados em linha por coluna:
            \[ A_{m \times n} \cdot B_{n \times p} = C_{m \times p} \ | \ c_{i \; j} = \displaystyle\sum_{k=1}^{n} {a_{i \; k} \cdot b_{k \; j}} \]
            Nota-se que: $ M_{y \times x} \cdot I_y = I_y \cdot M_{y \times x} = M_{y \times x} $
        \end{description}
    %--- 5.1.5
    \subsubsection{Transposição}
        A transposição de uma matriz é seu espelhamento, invertendo a indexação de linhas e colunas de seus elementos:
        \[ M_{y \times x} = a_{i \; j} \xrightarrow{\text{transposição}} M_{x \times y}^t = a_{j \times i} \]
        Frente ao processo de transposição matricial, surgem duas classificações:
        \begin{description}
            \item[Matriz Simétrica:] é toda matriz quadrada equivalente a sua transposição.
            \item[Matriz Antissimétrica:] é toda matriz quadrada cuja qual, quando transposta, resulta na sua matriz oposta.
        \end{description}
        %--- 5.1.6
        \subsubsection{Inversão}
        Uma matriz quadrada é dita inversível quando existe uma inversa que multiplicada por ela resulta numa matriz identidade de mesma ordem:
        \[ M_{n \times n} \cdot M_{n \times n}^{-1} = M_{n \times n}^{-1} \cdot M_{n \times n} = I_n \]
%------- 5.2
\subsection{Determinantes}
    %--- 5.2.1
    \subsubsection{Definição}
        O determinante é uma função matricial que associa matrizes quadradas a um escalar que a define e mede sua transformação no espaço vetorial, possuindo inúmeras aplicações em campos diversos da matemática e física. Para matrizes de primeira e segunda ordem, o determinante é encontrado da seguinte forma:
        \[ \det{M_{1 \times 1}} = \begin{vmatrix} a_{1 \; 1} \end{vmatrix} = a_{1 \; 1} \]
        \begin{center}
            \begin{tikzpicture}
                \begin{pgfonlayer}{fg}
                    \matrix (M) [matrix of math nodes, left delimiter  = |, right delimiter = |] at (0,0) { a & b  \\ c & d \\ } node [left=20pt] {$ \det{M_{2 \times 2}} = $} node[right=20pt] {$ = ad - bc $};
                \end{pgfonlayer}
                \begin{pgfonlayer}{main}
                    \node[circle, fill=white] at (M-1-1) {};
                    \node[circle, fill=white] at (M-1-2) {};
                    \node[circle, fill=white] at (M-2-1) {};
                    \node[circle, fill=white] at (M-2-2) {};
                \end{pgfonlayer}
                \begin{pgfonlayer}{bg}
                    \draw[blue, ->] (0.4,0.4) -- (-0.4,-0.5);
                    \draw[red, ->] (-0.4,0.4) -- (0.4,-0.5);
                \end{pgfonlayer}
            \end{tikzpicture}
        \end{center}
    %--- 5.2.2
    \subsubsection{Regra de Sarrus}
        Para matrizes de terceira ordem, usamos a seguinte regra:
        \begin{center}
            \begin{tikzpicture}
                \begin{pgfonlayer}{fg}
                    \matrix (M) [matrix of math nodes, left delimiter  = |, right delimiter = |] at (0,0) { a & b & c \\ d & e & f \\ g & h & i \\ } node [left=30pt] {$ \det{M_{3 \times 3}} =  $} node[right=30pt] {$ = (aei + bfg + cdh) - (ceg + bdi + afh) $};
                \end{pgfonlayer}
                \begin{pgfonlayer}{main}
                    \node[circle, fill=white] at (M-1-1) {};
                    \node[circle, fill=white] at (M-1-2) {};
                    \node[circle, fill=white] at (M-1-3) {};

                    \node[circle, fill=white] at (M-2-1) {};
                    \node[circle, fill=white] at (M-2-2) {};
                    \node[circle, fill=white] at (M-2-3) {};

                    \node[circle, fill=white] at (M-3-1) {};
                    \node[circle, fill=white] at (M-3-2) {};
                    \node[circle, fill=white] at (M-3-3) {};
                \end{pgfonlayer}
                \begin{pgfonlayer}{bg}
                    \draw[blue, ->] (0.6,0.7) -- (-0.6,-0.75);
                    \draw[blue, -] (0.15,0.7) -- (-0.6,-0.15);
                    \draw[blue, -] (-0.3,0.7) -- (-0.6,0.35);
                    \draw[blue, ->] (0.6,0.25) -- (-0.2,-0.75);
                    \draw[blue, ->] (0.6,-0.3) -- (0.3,-0.75);

                    \draw[red, ->] (-0.6,0.7) -- (0.6,-0.75);
                    \draw[red, -] (-0.15,0.7) -- (0.6,-0.15);
                    \draw[red, -] (0.3,0.7) -- (0.6,0.35);
                    \draw[red, ->] (-0.6,0.25) -- (0.2,-0.75);
                    \draw[red, ->] (-0.6,-0.3) -- (-0.3,-0.75);
                \end{pgfonlayer}
            \end{tikzpicture}
        \end{center}
    %--- 5.2.3
    \subsubsection{Menor Complementar}
        Para uma matriz de ordem maior que 2, o complementar algébrico de um elemento é o determinante que se obtém ao suprimir a linha e coluna desse elemento. \eg
        \[ M_{3 \times 3} = \begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \\ \end{vmatrix} : D_{1 \; 1} =  \begin{vmatrix} e & f\\ h & i\end{vmatrix}; D_{2 \; 2} =  \begin{vmatrix} a & c\\ g & i \end{vmatrix}; D_{3 \; 3} =  \begin{vmatrix} a & b \\ d & e \end{vmatrix} \]
        %--- 5.2.4
        \subsubsection{Complemento Algébrico}
        Para uma matriz de ordem maior que 2, o complemento algébrico (cofator) de um elemento é dado por:
        \[ A_{i \; j} = (-1)^{i + j} \cdot D_{i \; j} \]
    %--- 5.2.5
    \subsubsection{Determinante por Recorrência}
        A definição da determinante por recorrência (caso geral) é dada pela soma dos produtos dos elementos da primeira coluna por seus complementos algébricos:
        \[ \det{M_{n \times n}} = \displaystyle\sum_{i=i}^{n} {a_{i \; 1} \cdot A_{i \; 1} } \]
    %--- 5.2.6
    \subsubsection{Teorema de Laplace}
        O teorema de Laplace diz que o determinante de uma matriz de ordem maior que 2 é a soma dos produtos dos elementos de uma fila qualquer por seus respectivos cofatores, assim a definição do determinante por recorrência não precisa ser aplicado necessariamente à primeira coluna, mas a qualquer linha ou coluna de uma matriz.
    %--- 5.2.7
    \subsubsection{Combinação Linear}
        Define-se pelo conjunto das somas dos produtos dos elementos de determinadas filas por determinadas constantes, ordenadamente:
        \[ M_{n \times n} = [a_{i \; j}] \hphantom{---} s=\{filas \: paralelas \} \hphantom{---} p = \# s \hphantom{---} c = \{ c_1,...,c_p \}\]
        \[ \alpha = \{ \alpha_1, ..., \alpha_n \} \ | \ \alpha_{\sigma} =  \begin{dcases}  \displaystyle\sum_{k=1}^{p} {a_{s_k \; \sigma} \cdot c_k} \because s = \{linhas\} \\ \displaystyle\sum_{k=1}^{p} {a_{\sigma \; s_k} \cdot c_k} \because s = \{colunas\} \end{dcases} \]

        \eg
        \[ M_{3 \times 3} = [a_{i \; j}] \]
        \[ s_{x} = \{ 2,3 \}, c=\{q,r\} \rightarrow \alpha = \begin{dcases} \alpha_1 = a_{2 \; 1} \cdot q + a_{3 \; 1} \cdot r \\ \alpha_2 = a_{2 \; 2} \cdot q + a_{3 \; 2} \cdot r \\ \alpha_3 = a_{2 \; 3} \cdot q + a_{3 \; 3} \cdot r \end{dcases} \]
        \[ s_{y} = \{1,2,3\}, c=\{q,r,s\} \rightarrow \alpha = \begin{dcases} \alpha_{1} = a_{1 \; 1} \cdot q + a_{1 \; 2} \cdot r + a_{1 \; 3} \cdot s \\ \alpha_{2} = a_{2 \; 1} \cdot q + a_{2 \; 2} \cdot r + a_{2 \; 3} \cdot s \\ \alpha_{3} = a_{3 \; 1} \cdot q + a_{3 \; 2} \cdot r + a_{3 \; 3} \cdot s \end{dcases} \]
    %--- 5.2.8
    \subsubsection{Propriedades}
        \begin{description}
            \item[P-1:] o determinante de uma matriz é igual ao de sua transposta:
            \[ \det{M} = \det{M^t} \]
            \item[P-2:] se os elementos de uma fila qualquer de uma matriz forem todos nulos, então o determinante dessa matriz é zero.
            \item[P-3:] se multiplicada uma fila qualquer de uma matriz por uma constante, o valor do determinante dessa matriz também é multiplicado pela mesma constante. \eg
            \[ k \cdot \begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix} = \begin{vmatrix} a & kb & c \\ d & ke & f \\ g & kh & i \end{vmatrix} \]
            \[ \det{(k \cdot M_{n \; n})} = k^n \cdot \det{M_{n \; n}} \]
            \item[P-4:] se invertidas as posições de 2 filas paralelas de uma matriz, o determinante da nova matriz formada será o oposto do determinante da primeira. \eg
            \[ \begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix} = -1 \cdot \begin{vmatrix} b & a & c \\ e & d & f \\ h & g & i \end{vmatrix} \]
            \item[P-5:] se uma matriz tem 2 filas paralelas idênticas, por consequência da P-4 o determinante será 0.
            \item[P-6:] a soma dos produtos dos elementos de uma fila qualquer pelos cofatores de uma fila paralela resulta em 0:
            \[ A_{n \times n} = [a_{i \; j}] \hphantom{---} (p \in \mathbb{N}^n - i) \hphantom{---} (q \in \mathbb{N}^n - j) \]
            \[ \sum_{i=1}^{n} {a_{i \; j} \cdot A_{p \; j}} = \sum_{j=1}^{n} {a_{i \; j} \cdot A_{i \; q}} = 0 \]
            \item[P-7:] se duas filas paralelas de uma matriz forem formadas por elementos respectivamente proporcionais, então seu determinante será 0:
            \[ (A_{n \times n} = [a_{i \; j}]) \hphantom{---} (p \in \mathbb{N}^n - i) \hphantom{---} (q \in \mathbb{N}^n - i) \hphantom{---} (k = \{ \alpha \in \mathbb{R} \ | \ \#k = n \}) \]
            \[ \begin{matrix} a_{i \; j} = a_{p \; j} \cdot k_j \\ a_{i \; j} = a_{i \; q} \cdot k_i \end{matrix} \bigg{\}} \rightarrow \det{A} = 0 \]
            \item[P-8:] caso tomada uma matriz e decompostos os elementos de uma fila qualquer em uma soma de dois elementos, o determinante dessa matriz será a soma dos determinantes das matrizes formadas pela substituição dessa fila pelas parcelas da soma. \eg
            \[ \begin{vmatrix} a & (b_1 + b_2) & c \\ d & (e_1 + e_2) & f \\ g & (h_1 + h_2) & i \end{vmatrix} = \begin{vmatrix} a & b_1 & c \\ d & e_1 & f \\ g & h_1 & i \end{vmatrix} + \begin{vmatrix} a & b_2 & c \\ d & e_2 & f \\ g & h_2 & i \end{vmatrix} \]
            \item[P-9:] se uma matriz quadrada tiver uma de suas filas como a combinação linear de suas outras filas, então o determinante dessa matriz é 0. \eg
            \[ \begin{vmatrix} 0 & 1 & 2 \\ 2 & 3 & 8 \\ 4 & 5 & 14 \end{vmatrix} = 0 \because \{a_{1 \; 3}, a_{2 \; 3}, a_{3 \; 3}\} = \{\alpha_1, \alpha_2, \alpha_3\} \ | \ s_y = \{1,2\}, c=\{1,2\} \]
            \item[P-10:] se uma fila qualquer de uma matriz quadrada for multiplicada por uma constante e somada a outra fila paralela, o determinante se mantém inalterado. \eg
            \[ \begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix} = \begin{vmatrix} a & (b + ka) & c \\ d & (e + kd) & f \\ g & (h + kg) & i \end{vmatrix} \]
            \item[P-11:] o determinante de uma matriz triangular é igual ao produto dos elementos da diagonal principal. \eg
            \[ \begin{vmatrix} a & 0 & 0 \\ d & e & 0 \\ g & h & i \end{vmatrix} = a \cdot e \cdot i \]
            \item[P-12:] dadas duas matrizes quadradas de mesma ordem, o determinante do produto das matrizes é igual ao produto dos determinantes das matrizes:
            \[ A_{n \times n}, B_{n \times n} \leftarrow \det{(A \cdot B)} = \det{A} \cdot \det{B} \]
        \end{description}
    %--- 5.2.9
    \subsubsection{Redução da Ordem}
        Usando o teorema de Jacobi (P-10) e o teorema de Laplace, é possível reduzir a ordem de um determinante, desde que o primeiro elemento da primeira coluna da matriz tenha valor 1:
        \[ A_{n \times n} = \begin{bmatrix} 1 & a_{1 \; 2} & \cdots & a_{1 \; n} \\ a_{2 \; 1} & a_{2 \; 2} & \cdots & a_{2 \; n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n \; 1} & a_{n \; 2} & \cdots  & a_{n \; n} \end{bmatrix} \]
        
        Soma-se a primeira coluna a cada uma das restantes, multiplicadas pelo oposto de seu respectivo primeiro elemento:
        \[ B_{n \times n} = [b_{i \; j}] \ | \ \begin{dcases} b_{1 \; 1} = 1 \\ b_{i \; j} = a_{i \; j} + a_{i \; 1} \cdot (-a_{1 \; j}) \forall i \in \mathbb{N}^n, j \in \mathbb{N}^n -1 \end{dcases} \]
        \[ B = \begin{bmatrix} 1 & (a_{1 \; 2} -1 a_{1 \; 2}) & \cdots & (a_{1 \; n} -1 a_{1 \; n}) \\ a_{2 \; 1} & (a_{2 \; 2} - a_{2 \; 1} \cdot a_{1 \; 2}) & \cdots & (a_{2 \; n} - a_{2 \; 1} \cdot a_{1 \; n}) \\ \vdots & \vdots & \ddots & \vdots \\ a_{n \; 1} & (a_{n \; 2} - a_{n \; 1} \cdot a_{1 \; 2}) & \cdots  & (a_{n \; n} - a_{n \; 1} \cdot a_{1 \; n}) \end{bmatrix} = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ b_{2 \; 1} & b_{2 \; 2} & \cdots & b_{2 \; n} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n \; 1} & b_{n \; 2} & \cdots  & b_{n \; n} \end{bmatrix} \]
        
        Por fim, aplicando o teorema de Laplace:
        \[ \det{A} = \det{B} = \begin{vmatrix} b_{2 \; 2} & \cdots & b_{2 \; n} \\ \vdots & \ddots & \vdots \\ b_{2 \; n} & \cdots & b_{n \; n} \end{vmatrix} \]
    %--- 5.2.10
    \subsubsection{Regra de Chió}
        Simplificando o processo de redução de ordem de uma matriz, a regra de Chió diz que, sendo o primeiro elemento de uma matriz quadrada igual a 1, encontra-se uma matriz de ordem imediatamente menor com igual determinante suprimindo a primeira linha e coluna da matriz inicial, e em seguida subtraindo cada elemento do produto dos extremos de sua posição:
        \[ A_{n \times n} = [a_{i \; j}] \ | \ a_{1 \; 1} = 1 \hphantom{---} B_{n-1 \times n-1} = [b_{i \; j}] \ | \ b_{i \; j} = a_{(i+1) \; (j+1)} - a_{(i+1) \; 1} \cdot a_{1 \; (j+1)} \]
        \[ \det{A} = \det{B} \]
        
        \eg
        \[ \begin{vmatrix} 1 & b & c \\ d & e & f \\ g & h & i \end{vmatrix} = \begin{vmatrix} (e - bd) & (f-cd) \\ (h -bg) & (i -cg) \end{vmatrix} \]
    %--- 5.2.11
    \subsubsection{Matriz de Vandermonde}
        Também conhecida como matriz das potências, matriz de Vandermonde é toda aquela que se define por filas paralelas de progressões geométrica com seus primeiros elementos iguais a um, e segundos elementos chamados característicos. Essas matrizes têm seu determinante encontrado pelo prodututório do arranjo das diferenças entre os elementos característicos:
        \[ V_{n \times n} = [v_{i \; j}] \ | \ \begin{dcases} a_{i \; j} = a_{2 \; j}^{i-1} \\ a_{i \; j} = a_{i \; 2}^{j-1} \end{dcases} \]
        \[ \det{V_x} = \displaystyle\prod_{1 \leq p < q \leq n} {v_{2 \; q} - v_{2 \; p}} \hphantom{---} \det{V_y} = \displaystyle\prod_{1 \leq p < q \leq n} {v_{q \; 2} - v_{p \; 2}} \]

        \eg
        \[ \begin{vmatrix} a^0 & b^0 & c^0 \\ a^1 & b^1 & c^1 \\ a^2 & b^2 & c^2 \end{vmatrix} = (b^1 - a^1) \cdot (c^1 - a^1) \cdot (c^1 - b^1) \]
    %--- 5.2.12
    \subsubsection{Matriz Adjunta}
        Chama-se matriz adjunta aquela obtida pela substituição dos elementos da matriz inicial por seus respectivos complementos algébricos, seguida da transposição da matriz:
        \[ M_{n \times n} = [a_{i \; j}] \hphantom{---} \overline{M}_{n \times n} = [A_{j \; i}] \]
        
        Com a aplicação dos teoremas de Laplace e Cauchy, encontra-se uma propriedade sobre o determinante da matriz inicial, e por consequência dessa, é possível calcular a sua inversa:
        \[ M \cdot \overline{M} = \overline{M} \cdot M = \det{M} \cdot I_ n \]
        \[ M^{-1} = \overline{M} \cdot \frac{1}{\det{M}} \]
%------- 5.3
\subsection{Sistemas Lineares}
    %--- 5.3.1
    \subsubsection{Equação Linear}
        É uma equação formada pela soma do produto de diferentes incógnitas por seus respectivos coeficientes, resultando no chamado termo independente. Uma ênupla ordenada de números reais é dada como solução de uma equação linear quando esta a satisfaz:
        \[ (a, b \in \mathbb{R}), (i, n \in \mathbb{N}) \]
        \[ \rho = \displaystyle\sum_{i=1}^{n} {a_i x_i} = a_1 x_1 + \cdots + a_n x_n = b \]
        \[ S_{\rho} = \{\alpha_1, \cdots, \alpha_n\} \leftrightarrow \displaystyle\sum_{i=1}^{n} {a_i \alpha_i} = b \]
    %--- 5.3.2
    \subsubsection{Sistema Linear}
        É um conjunto de equações lineares nas mesmas incógnitas, tendo uma representação em sua forma matricial. Uma ênupla ordenada só é dada como solução de um sistema linear se for solução de todas as suas equações lineares:
        \[ P = \begin{bmatrix} a_{1 \; 1} & \cdots & a_{1 \; n} \\ \vdots & \ddots & \vdots \\ a_{m \; 1} & \cdots & a_{m \; n} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} b_1 \\ \vdots \\ b_m \end{bmatrix} \]

        Quando a forma matricial só carrega os coeficientes, é chamada de matriz incompleta, e quando a última coluna contém os termos independentes, é a chamada matriz completa:
        \[ \begin{bmatrix} a_{1 \; 1} & \cdots & a_{1 \; n} & r_1 \\ \vdots & \ddots & \vdots & \vdots \\ a_{m \; 1} & \cdots & a_{m \; n} & r_m \end{bmatrix} \]
    %--- 5.3.3
    \subsubsection{Sistema Linear Homogêneo}
        É definido como um sistema onde todos os termos independentes são iguais a zero, sendo sempre um sistema possível pela chamada solução nula, onde todos os coeficientes são zero.
    %--- 5.3.4
    \subsubsection{Teorema de Cramer}
        Considerando um sistema linear com o número de equações igual ao número de incógnitas, formando assim uma matriz quadrada, o sistema será possível e determinado caso o determinante dessa matriz seja diferente de zero:
        \[ P_i = [a_{y \; x}] \ | \ a_{y \; x} = \begin{dcases} b_{y \; i} \\ a_{y \; x} \ | \ x \neq i \end{dcases} \]
        \[ S_P = \{ \alpha_1, \cdots, \alpha_n \} \ | \ \alpha_i = \frac{\det{P_i}}{\det{P}} \leftrightarrow \det{P} \neq 0 \]
    %--- 5.3.5
    \subsubsection{Sistema Escalonado}
        Quando um dado sistema tem o número de coeficientes nulos antes do primeiro não nulo aumentando de equação para equação, este é chamado de sistema escalonado. Para tal caso, se sua matriz incompleta for triangular, seu determinante sempre será diferente de zero e assim se aplica o teorema de Cramer, mas caso hajam mais incógnitas do que equações o sistema será possível e indeterminado. \eg
        \[ P = \begin{bmatrix} a_{1 \; 1} & a_{1 \; 2} & a_{1 \; 3} \\ 0 & a_{2 \; 2} & a_{2 \; 3} \\ 0 & 0 & a_{3 \; 3} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix} \]
    %--- 5.3.6
    \subsubsection{Escalonamento de um Sistema}
        O escalonamento de um sistema se baseia em reduzir as incógnitas linha-a-linha montando sistemas equivalentes com base em operações elementares sobre linhas:
        \begin{description}
            \item[P-1:] trocadas as posições de duas equações, o sistema se mantém equivalente;
            \item[P-2:] multiplicada uma equação de um sistema por uma constante, o sistema se mantém equivalente;
            \item[P-3:] substituída uma equação pela mesma somada a outra do sistema, este se mantém equivalente.
        \end{description}
    %--- 5.3.7
    \subsubsection{Teorema de Rouché-Capelli}
        Um sistema só será possível caso o número de linhas não nulas de sua matriz completa for igual ao de sua matriz completa.
%------- 5.4
\subsection{Vetores}
    \[ \Upsilon \pi o \mu o \nu \eta \]